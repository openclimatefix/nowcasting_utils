{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7399cff4",
   "metadata": {},
   "source": [
    "## Convert ESO results\n",
    "\n",
    "Idea is to convert to raw ESO results into the same format than the outputs from OCF ML models\n",
    "\n",
    "### Load ESO file\n",
    "\n",
    "Load the ESO file\n",
    "\n",
    "### Load ESO GSP metadata\n",
    "\n",
    "Load the ESO GSP metadata. We will need to convert between GSP ID and GSP name, and know the location of each GSP\n",
    "\n",
    "### Load test dataset \n",
    "\n",
    "Let load the test dataset meta file. This is so we know which times and what locations / gsps to compare\n",
    "\n",
    "### Reduce ESO forecasts \n",
    "\n",
    "Reduce ESO forecast to the same as the test dataset\n",
    "\n",
    "### Load Forecast Truth and Capacity\n",
    "\n",
    "Load the GSP outturn value at each forecast value. Also load the capacity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f840a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# location fo ESO forecasts\n",
    "ESO_PV_FORECASTS_PATH = Path(\"/mnt/storage_b/data/ocf/solar_pv_nowcasting/other_organisations_pv_forecasts/National_Grid_ESO/NetCDF/ESO_GSP_PV_forecasts.nc\")\n",
    "\n",
    "# The locations of the tests dataset\n",
    "TEST_DATASET_FILE ='/mnt/storage_ssd_4tb/data/ocf/solar_pv_nowcasting/nowcasting_dataset_pipeline/prepared_ML_training_data/v16/test/spatial_and_temporal_locations_of_each_example.csv'\n",
    "\n",
    "# The \"ground truth\" estimated total PV generation from each Grid Supply Point from Sheffield Solar:\n",
    "GSP_ZARR_PATH = Path(\"/mnt/storage_b/data/ocf/solar_pv_nowcasting/nowcasting_dataset_pipeline/PV/GSP/v3/pv_gsp.zarr\")\n",
    "\n",
    "# Output csv\n",
    "ESO_PV_FORECASTS_OUTPUT_FILE = Path(\"/mnt/storage_b/data/ocf/solar_pv_nowcasting/other_organisations_pv_forecasts/National_Grid_ESO/CSV/testset_v16.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc27688",
   "metadata": {},
   "source": [
    "### Load ESO file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92df06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load main file\n",
    "\n",
    "import xarray as xr\n",
    "\n",
    "eso_pv_forecasts_dataset = xr.open_dataset(ESO_PV_FORECASTS_PATH)\n",
    "eso_pv_forecasts_dataset = eso_pv_forecasts_dataset.rename({'gsp_id': 'gsp_name'})\n",
    "eso_pv_forecasts_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be680fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Select just two timesteps: 30 minutes to 2 hours ahead:\n",
    "ESO_FORECAST_ALGO_NAME = 'ASL'  # Either ASL or ML.\n",
    "selected_eso_forecasts_dataarray = (\n",
    "    eso_pv_forecasts_dataset[ESO_FORECAST_ALGO_NAME]\n",
    "    .sel(step=slice(pd.Timedelta(\"30 minutes\"), pd.Timedelta(\"2 hour\"))))\n",
    "\n",
    "selected_eso_forecasts_dataarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503af0ba",
   "metadata": {},
   "source": [
    "### Load ESO GSP metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecd8d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nowcasting_dataset.data_sources.gsp import eso\n",
    "from nowcasting_dataset.geospatial import lat_lon_to_osgb\n",
    "\n",
    "# Download metadata from ESO for each GSP.  We need this because\n",
    "# the GSP PV data from Sheffield Solar uses integer gsp_ids, whilst\n",
    "# the ESO forecasts use textual gsp_names.  So we need the metadata\n",
    "# to allow us to map from gsp_id to gsp_name.\n",
    "gsp_metadata = eso.get_gsp_metadata_from_eso(calculate_centroid=True)\n",
    "\n",
    "gsp_id_to_name = (\n",
    "    gsp_metadata[['gsp_id', 'gsp_name']]\n",
    "    .dropna()\n",
    "    .astype({'gsp_id': int})\n",
    "    .set_index('gsp_id')\n",
    "    .squeeze()\n",
    ")\n",
    "\n",
    "gsp_name_to_id = (\n",
    "    gsp_metadata[['gsp_id', 'gsp_name']]\n",
    "    .dropna()\n",
    "    .astype({'gsp_name': str})\n",
    "    .set_index('gsp_name')\n",
    "    .squeeze()\n",
    ")\n",
    "print(gsp_id_to_name)\n",
    "\n",
    "gsp_metadata[\"location_x\"], gsp_metadata[\"location_y\"] = lat_lon_to_osgb(\n",
    "            lat=gsp_metadata[\"centroid_lat\"], lon=gsp_metadata[\"centroid_lon\"]\n",
    "        )\n",
    "\n",
    "print(gsp_metadata.columns)\n",
    "print(gsp_metadata[['location_x','location_y']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d73eff",
   "metadata": {},
   "source": [
    "### Load test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa797c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of datetimes fof test set\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# load location for test dataset\n",
    "locations_df = pd.read_csv(TEST_DATASET_FILE)\n",
    "\n",
    "# append gsp id to locations_df\n",
    "\n",
    "# this causes allot of nans, so do it a loop\n",
    "# new_df = pd.merge(locations_df, gsp_metadata[['gsp_id','location_x','location_y']],  \n",
    "#                   how='left', left_on=['x_center_OSGB','y_center_OSGB'], \n",
    "#                   right_on = ['location_x','location_y'])\n",
    "\n",
    "locations_df['gsp_id'] = 0.0\n",
    "# loop over all the data points int he test set. Can take about 2 mins ~10,000 data points\n",
    "for i in tqdm(range(len(locations_df))):\n",
    "    x_meters_center = locations_df.x_center_OSGB[i]\n",
    "    y_meters_center = locations_df.y_center_OSGB[i]\n",
    "    \n",
    "    meta_data_index = gsp_metadata.index[\n",
    "            np.isclose(gsp_metadata.location_x, x_meters_center, rtol=1e-05, atol=1e-05)\n",
    "            & np.isclose(gsp_metadata.location_y, y_meters_center, rtol=1e-05, atol=1e-05)\n",
    "        ]\n",
    "    gsp_id = gsp_metadata.loc[meta_data_index].gsp_id.values[0]\n",
    "    locations_df.loc[i, 'gsp_id'] = gsp_id\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91bb63b",
   "metadata": {},
   "source": [
    "### Reduce ESO forecasts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f85879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only select the 'forecast_date_time' from test set\n",
    "\n",
    "# change them to have mins of 0 or 30, as ESO forecasts are at 0 or 30 mins. \n",
    "locations_df['t0_datetime_UTC_floor_30_mins'] = pd.to_datetime(locations_df['t0_datetime_UTC']).dt.floor('30T')\n",
    "forecast_date_time = selected_eso_forecasts_dataarray.forecast_date_time\n",
    "\n",
    "# make list of datetimes that are in 't0_datetime_utc' and 'forecast_date_time'\n",
    "t0_datetime_utc = pd.DatetimeIndex(locations_df['t0_datetime_UTC_floor_30_mins']).unique()\n",
    "forecast_date_time = pd.DatetimeIndex(forecast_date_time)\n",
    "forecast_date_time = forecast_date_time.join(t0_datetime_utc, how='inner')\n",
    "\n",
    "# select the eso forecasts that are in the test set\n",
    "\n",
    "print(forecast_date_time.max())\n",
    "print(forecast_date_time.min())\n",
    "selected_eso_forecasts_dataarray = selected_eso_forecasts_dataarray.sel(forecast_date_time=forecast_date_time)\n",
    "\n",
    "# also filter t0_datetimes\n",
    "locations_df = locations_df[locations_df['t0_datetime_UTC_floor_30_mins'].isin(forecast_date_time)]\n",
    "print(locations_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f491b72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dont want to load any unnecessary data, so going to loop through t0_datetime_UTC and only save the forecast we need\n",
    "\n",
    "eso_dataarrays_list =[]\n",
    "# this can take about 2 seconds for ~5000 samples\n",
    "for i in tqdm(range(len(locations_df))):\n",
    "    t0_datetime_UTC_floor_30_mins = locations_df.t0_datetime_UTC_floor_30_mins.iloc[i]\n",
    "    gsp_id = locations_df.gsp_id.iloc[i]\n",
    "    gsp_name = gsp_id_to_name[gsp_id]\n",
    "    \n",
    "    \n",
    "    \n",
    "    one_eso_forecasts_dataarray = selected_eso_forecasts_dataarray.sel(forecast_date_time=t0_datetime_UTC_floor_30_mins)\n",
    "    one_eso_forecasts_dataarray = one_eso_forecasts_dataarray.sel(gsp_name=gsp_name)\n",
    "    \n",
    "    eso_dataarrays_list.append(one_eso_forecasts_dataarray)\n",
    "    \n",
    "predictions = []\n",
    "# this can take about 20 seconds for ~5000 samples\n",
    "for eso_dataarrays in tqdm(eso_dataarrays_list):\n",
    "    \n",
    "    target_times = eso_dataarrays.forecast_date_time + eso_dataarrays.step\n",
    "    forecast = eso_dataarrays.values\n",
    "    gsp_id = gsp_name_to_id[eso_dataarrays.gsp_name.values]\n",
    "    \n",
    "    \n",
    "    predictions_df = pd.DataFrame({'forecast_gsp_pv_outturn_mw':forecast,\n",
    "                                   'target_datetime_utc':target_times})\n",
    "    predictions_df['gsp_id'] = int(gsp_id)\n",
    "    predictions_df['t0_datetime_utc'] = eso_dataarrays.forecast_date_time.values\n",
    "    \n",
    "    predictions.append(predictions_df)\n",
    "    \n",
    "    \n",
    "predictions = pd.concat(predictions)\n",
    "print(predictions)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d061731",
   "metadata": {},
   "source": [
    "### Load Forecast Truth and Capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30991851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need to find out the truth values\n",
    "pv_live_dataset = xr.open_dataset(GSP_ZARR_PATH, engine=\"zarr\")\n",
    "\n",
    "# Convert 'gsp_id' from strings like '1', '2', etc. to ints\n",
    "pv_live_dataset['gsp_id'] = pv_live_dataset['gsp_id'].astype(int)\n",
    "\n",
    "predictions_and_truths = predictions\n",
    "predictions_and_truths['actual_gsp_pv_outturn_mw'] = 0.0\n",
    "predictions_and_truths['capacity_mwp'] = 0.0\n",
    "# this can take about 1 min 30 seconds for ~5000 samples\n",
    "for i in tqdm(range(len(predictions_and_truths))):\n",
    "    \n",
    "    target_datetime_utc = predictions_and_truths.target_datetime_utc.iloc[i]\n",
    "    gsp_id = predictions_and_truths.gsp_id.iloc[i]\n",
    "    \n",
    "    one_pv_live_dataset = pv_live_dataset.sel(datetime_gmt=target_datetime_utc)\n",
    "    one_pv_live_dataset = one_pv_live_dataset.sel(gsp_id=gsp_id)\n",
    "    \n",
    "    predictions_and_truths.actual_gsp_pv_outturn_mw.iloc[i] = one_pv_live_dataset.generation_mw.values\n",
    "    predictions_and_truths.capacity_mwp.iloc[i] = one_pv_live_dataset.installedcapacity_mwp.values\n",
    "    \n",
    "print(predictions_and_truths)\n",
    "    \n",
    "\n",
    "# is genration mw normalized?\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2b241a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file to csv\n",
    "predictions_and_truths.to_csv(ESO_PV_FORECASTS_OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12eb19a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e351c46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80debdcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
